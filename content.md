# メモコンテンツ（共有チャット全文）

## 会話ログ

あなた:
本業の方でcopilotを活用して顧客向けのweb画面のシステムテストを効率化したい。web画面はterasoluna webのためオーソドックスな作りだが、金融関連のシステムのため、実施したシナリオや実施結果の証跡、検証精度などが求められる。反面、試験項目は暗黙知で人が補いながら実施する箇所もあり、属人的な要素もある。設計書はある程度の揃っておりエクセル形式である。自動化のプランとして現在の試験のどの範囲を確実に行えるか？実行する場合の主なタスク、人とcopilotが担当する部分は何か、使用するシステムの構成案などを示して欲しい。環境としてwindows PCからインターネットは利用可能だが、設計書類は外部には出せない。copilotは企業向けのライセンスを導入済みだが、チャット程度しか使った経験がなく、これらの必要なサービスの種類も知りたい。設計書はgitではあるが、ファイルサーバ上に本体が置かれており、仕事の主体はファイルサーバで行っている。DBなどもsshのみだがリモートで操作は可能。何か質問があれば先に聞いて欲しい。

ChatGPT:
思考時間: 17s
まずこちらからプランを出しますね（質問は途中に織り込みません）。
前提として「設計書は社外に出せない」「Copilot は既にエンタープライズ系ライセンスあり」「Windows 端末＋社内ファイルサーバ＋DB は SSH で操作可」という状況で考えます。

1. どの範囲のテストを「確実に」Copilot で自動化しやすいか
1-1. 自動化と相性が良い範囲
A. 画面の正常系・代表的な異常系の回帰試験
例：
取引登録 → 確認画面 → 完了画面
必須項目未入力、桁数オーバー、禁則文字などのバリデーション
特徴：
入力 → 期待メッセージ／期待遷移 が Excel 仕様に比較的明示されている
UI操作はパターン化しやすい（Terasoluna の標準画面遷移）
👉 ここは Selenium / Playwright などの UI テストコードを、Copilot に書かせる・修正させる範囲。
B. 画面操作後の DB 検証（CRUD）
例：
取引登録後に DB に正しいレコードが INSERT/UPDATE されているか
ステータス更新や履歴テーブルへの記録
特徴：
期待値は Excel 設計書や ER 図などから決まっている
SQL 自体はパターン化しやすい
👉 Copilot に
「◯◯テーブルから主キー XX で登録したレコードを取得する SQL を書いて」
などと指示して、SQL 作成やテストコード埋め込みを自動化しやすい範囲。
C. テスト証跡の自動取得・整形
UIテスト自動化ツールでできること：
実行ログ（stepごとのPASS/FAIL）
スクリーンショット保存（異常時・任意タイミング）
Copilot でできること：
ログ＋スクリーンショットパスから 試験結果の Excel/Markdown を自動生成するスクリプトの作成
「試験項目ID」「実行日」「環境」「結果」「証跡ファイルパス」などを、自動整形して出力するコードを生成
👉 証跡作成の自動化・半自動化は、金融系の監査対応にかなり効きます。

1-2. Copilot だけでは難しく、人の関与が必須な範囲
D. 暗黙知依存の観点テスト
例：
画面上の文言のニュアンス（顧客が混乱しないか）
画面遷移のタイミングの違和感（業務フローとして自然か）
例外的な運用（お客さま都合の一部キャンセルなど）の線引き
👉 ここは「テスト観点を人が言語化 ⇒ 一部を形式化して Copilot に落とす」のが現実的。
完全自動化ではなく「観点リスト化・抜け漏れチェック」に Copilot を使うイメージ。
E. 新機能の探索的テスト
仕様書に書ききれていない挙動、UI の“気持ち悪さ”を探すテスト。
Copilot には「こんな観点もある」とヒントを出させることはできるが、実施・判断・記録は人が行う必要あり。
👉 探索テストは人が主体、Copilot は観点の洗い出し補助と割り切る。

2. 主なタスクと「人 vs Copilot」の役割分担
ざっくりフェーズに分けます。
フェーズ0：現状整理・ポリシー確認
人がやる：
現在のテストの実態整理
どの Excel が「設計書」「項目表」「結果表」か
どの画面・どの機能が優先度高いか（クリティカル機能）
セキュリティポリシー確認
Copilot に渡して良いのは「どこまでの情報か」（システム名・テーブル名・画面IDなどの固有名詞可否）
Copilot の役割：
人が作成したメモ（要件、制約など）を渡し、「ではテスト自動化のステップを整理して」と指示して ToDo リストや WBS のたたきを作らせる。
フェーズ1：テスト設計書（Excel）の構造分析と変換
人がやる：
代表的なテスト仕様 Excel を 1〜2ファイル選び、試験項目ID・画面ID/画面名・入力値・期待結果（画面表示／DB）など列構成を確認。
「このExcelの列構造だけ」を抜き出して、Copilot に共有可能な抽象化を作る（例：[No, Screen, TestID, Input, ExpectedMessage, ExpectedDB]）。
Copilot がやる：
抽象化したサンプルを元に、Gherkinシナリオ（Given-When-Then）や JSON / CSV 形式のテストデータなど、自動化しやすいフォーマットへの変換規則の案を作成。「このフォーマットから Playwright/Selenium テストコードを生成するスクリプト」のひな形を作る。
フェーズ2：UI自動テスト基盤の構築
人がやる：
使用ツールの選定・環境構築（例：Playwright（推奨） or Selenium + TestNG/JUnit）
Windows 端末 or CIサーバ（Jenkins, Azure DevOps 等）が使えるか判断
プロジェクト作成（ui-tests 的な専用リポジトリ）
ファイルサーバ上の仕様 Excel とは論理的に紐付ける（パス / ID）
Copilot がやる：
テンプレートコード生成（ログイン処理の共通メソッド、メニュー遷移の共通メソッド、テストデータを読み込んで画面に入力する共通関数など）
例：「Terasoluna の共通レイアウトで、メニューリンクをクリックして画面IDで遷移する関数を書いて」といった指示で土台のコードを生成。
フェーズ3：代表シナリオの自動化（PoC）
人がやる：
クリティカルな 1〜2 画面を選ぶ（例：ログイン、メイン取引登録）
その画面の Excel 仕様からまず 5〜10 項目程度を選び、「これは自動化対象」「これは人が目で確認」と線引きする。
Copilot がやる：
人が選んだテスト項目を元に、テストデータ定義ファイルの作成、Playwright/Selenium のテストコード生成
例：「このテーブル（テスト項目）をもとに、入力と期待メッセージを検証するテストケースを生成して」と指示。
DB 検証が必要なら：ER情報とサンプル SQL（人が1つ書く）を入力し、「他のケース用のSQLも生成して」と頼む。
フェーズ4：証跡自動出力 & レポート整形
人がやる：
証跡の要件を整理（試験項目ID・実施日時・環境・実行者・結果（○/×）・NGならスクリーンショットパス）
最終的に監査で見せる Excel のフォーマットを決める。
Copilot がやる：
テスト実行ログ＋スクリーンショット一覧から「試験結果 CSV/Excel」を吐き出すツールのコード生成。Markdown 形式で日次実行サマリ、失敗ケース一覧＋リンクを生成するスクリプトも作らせる。
👉 これで、「エビデンス・証跡」の作成がかなり定型化できます。
フェーズ5：回帰テストサイクルへの組み込み
人がやる：
リリースフローに「ブランチ〇〇にマージ → CIで UI テスト自動実行 → 結果をファイルサーバの特定フォルダへ送る」といった運用ルールを作る。
「どのタイミングで探索的テストを行うか」を決める（例えば UAT直前）。
Copilot がやる：
CI の設定ファイル（Jenkinsfile, Azure DevOps pipeline YAML 等）の雛形生成。リリースチェックリストのテンプレ作成（「自動テスト結果 OK」「探索テスト実施済」「証跡保管済」など）。

3. 使用する Copilot / システム構成のイメージ
3-1. Copilot 側のサービス種別イメージ
組織によって名称違いますが、ざっくりこんな使い分けです：
GitHub Copilot（IDE用）
VS Code / JetBrains で使用。主な用途：テストコード生成、SQL 作成、ログ整形ツールのコーディング（あなたのケースでは 最重要）。
Copilot Chat（GitHub / IDE / ブラウザ）
コードや設計の説明、WBSのたたき作成など。仕様書の「構造」を要約させたり、観点洗い出しに利用（機密情報ルールを守りつつ）。
Microsoft 365 Copilot（もし利用可なら）
Excel / Word / Teams と連携。試験項目表の整形、結果一覧の集計・グラフ化、日報・週報の自動ドラフトに便利。
Copilot Studio / Power Automate（ある場合）
ファイルサーバ上の特定フォルダに結果ファイルが置かれたら、自動で通知 or 集計、などのワークフロー構築に使える。
3-2. システム構成案（シンプル版）
開発・テスト実行環境：Windows 端末上で VS Code + GitHub Copilot、Playwright/Selenium テストプロジェクト、テスト対象 Web（Terasoluna）はテスト環境URLに接続。
リポジトリ：社内 Git（オンプレ or クラウド）に ui-test リポジトリ。testdata フォルダに Excelから変換した CSV/JSON（機密レベル注意）、src フォルダに Copilot と協働して作るテストコード。
ファイルサーバ：従来どおり仕様 Excel を置く。自動テストの結果証跡（スクリーンショット、ログ、結果 Excel）格納フォルダも用意。
CI サーバ（あれば）：Jenkins などがあれば毎晩 or リリース前に UI テスト自動実行。成果物をファイルサーバへコピー。

4. まとめ：最初の一歩としてやると良いこと
対象機能を絞る（「ログイン＋メイン取引1画面」を PoC 対象に）。
Excel テスト仕様の最小セットを決める（10ケース程度を「自動化候補」として抜き出す）。
VS Code + Copilot で Playwright or Selenium プロジェクトを作る（「TerasolunaベースのJava Webアプリのログイン画面を、Playwright で自動テストするテンプレートを書いて」と投げてみる）。
証跡フォーマットを決める（最終的に監査で見せる Excel のフォーマットを決めておく）。
Copilot に「人が決めたルール」をコード化させる（入力→実行→スクリーンショット→結果CSV出力まで一気通貫のスクリプト）。
もし次のステップとしてよければ、Playwright / Selenium どちらを選ぶかや、実際のテスト仕様 Excel の列構成に合わせた「変換フォーマット（CSV/JSON）」のサンプルをコード例付きで出します。

あなた:
この試験のシステム構成をDFDでplantumlで出力して欲しい

ChatGPT:
思考時間: 22s
```
@startuml
title システムテスト自動化 DFD（概念レベル）

' 外部エンティティ
actor "テスター\n(試験担当者)" as Tester
cloud "GitHub Copilot\n(クラウド)" as Copilot

' プロセス
rectangle "テスト設計・制御\n(Windows PC + VS Code)" as Design
rectangle "UIテスト実行\n(Playwright / Selenium)" as Runner
node "CIサーバ\n(Jenkins 等)" as CI

' データストア
database "業務Webアプリ\n(Terasoluna ベース)" as App
database "DBサーバ" as DB
database "ファイルサーバ\n(設計書 Excel)" as SpecFS
database "ファイルサーバ\n(証跡・結果)" as EvidenceFS

' データフロー：テスト設計・制御周り
Tester --> Design : 試験要求・観点\n実行指示
Design --> SpecFS : テスト仕様参照\n(Excel 読み取り)
SpecFS --> Design : テスト項目・期待結果\n(論理情報)

Design --> Copilot : コード生成指示\n(抽象化した仕様・構造情報)
Copilot --> Design : テストコード案\nSQL案・スクリプト案

Design --> Runner : テストスイート定義\n(テストコード/データ)
Runner --> Design : 実行結果概要\n(ケース毎の成否)

' データフロー：CI 経由の自動実行
CI --> Runner : 自動実行トリガ\n(ブランチマージ・定時実行)
Runner --> CI : 実行ステータス\n(成功/失敗・件数)

' データフロー：UI操作と DB 検証
Runner --> App : HTTP リクエスト\n(画面操作・入力値)
App --> DB : SQL 実行\n(INSERT/UPDATE/SELECT)
DB --> App : クエリ結果

Runner --> DB : 検証用 SELECT\n(期待レコード取得)
DB --> Runner : 検証結果\n(実データ)

' データフロー：証跡・結果
Runner --> EvidenceFS : ログ・スクリーンショット\n結果CSV/レポート
CI --> EvidenceFS : バッチ実行結果\n(日次/リリース前)

Tester --> EvidenceFS : 証跡参照\n(ログ・SS・結果一覧)

@enduml
```

あなた:
試験の効率化を人月で示して欲しい。現在試験の実施は15項目/人日程度、検証は30項目/人日程度である。試験項目作成などを元に適用前後の試算表で、検討価値がどの程度か関係者に説明して合意をしたい。

ChatGPT:
以下に 「試験効率化の投資対効果（人月換算）を説明するための試算表」 を示します。

あなた:
情報量が多すぎる。工数の内訳も少し考えて欲しい。試験項目作成、AIの場合はシナリオ作成、環境構築なども必要では？

ChatGPT:
了解しました。
情報量を大幅に絞り、試験効率化に関わる「工数の内訳」を最低限だけ整理し、AI 導入前後での “現実的な比較” を示します。
（ここでは PoC＋初期導入を含む全体像 をシンプルに描きます。）

■1. まず「試験業務の工数内訳」を最小構成で整理
金融系のシステムテストで典型的に発生する工数は以下の 5 つです。
工数区分内容
① 試験項目作成（設計書 ⇒ 試験項目）Excel 仕様から試験項目を起こす、観点を補う
② テストシナリオ化（順序・データ整備）入力順序、データパターン、DB前提データなど
③ テスト環境構築テスト自動化ツール、テンプレコード、実行環境準備
④ 試験実施（画面操作）実行手順に従って操作、入力、遷移の確認
⑤ 検証（画面・DB・証跡作成）結果確認、DB照合、スクリーンショット、記録
現在あなたが持っている数字（15項目/日、30項目/日）は「④・⑤」に該当します。AI 導入では ②、③も大きく効く 点が重要です。

■2. 人手の場合の工数モデル（年間 1,000 項目と仮定）
工数区分1,000項目あたりの目安備考
① 項目作成0.05日×1000 = 50人日1項目あたり3〜5分で作成として
② シナリオ化10人日項目グルーピング・入力順序整理
③ 環境構築5人日（最初だけ）手動実行ならほぼ不要
④ 試験実施1000/15 = 67人日15項目/日
⑤ 検証1000/30 = 33人日30項目/日

あなた:
表一つに凝縮して欲しい。スマホでなくパソコンで説明する。ダラダラと色々な資料を羅列しても伝わらない。一つの表とヘッドメッセージ、サマりしすぎて解像度がぼやけたように成らず、かつ無駄な表記がない価値のある表を出力して

ChatGPT:
以下、**1枚だけで説明できる“価値のある比較表”**を提示します。
**「試験業務の工数内訳 × AI 自動化前後の削減インパクト」**を一つの表に凝縮しました。

■ 試験業務の工数内訳と AI 導入効果（1,000 項目規模の例）
— 一覧だけで “どこに効くのか” が伝わる構成 —

| 工数区分                     | 人手（現状）           | AI・自動化後            | 削減率 | コメント（どこに効くか）                    |
|-----------------------------|-------------------------|--------------------------|--------|----------------------------------------------|
| ① 試験項目作成              | 50 人日                 | 25 人日                  | ▲50%  | Excel → 項目化の定型処理をAIが半自動化      |
| ② シナリオ化（順序/データ） | 10 人日                 | 4 人日                   | ▲60%  | 入力順・前提データの整理をAIが生成・補助     |
| ③ 自動化環境構築            | 5 人日（初回のみ）      | 8 人日（初回）           | +3 人日| 初回は投資増。ただし2回目以降ほぼゼロ        |
| ④ 試験実施（画面操作）      | 67 人日                 | 12 人日                  | ▲82%  | UI操作・遷移確認を自動実行                   |
| ⑤ 検証（画面/DB/証跡）      | 33 人日                 | 6 人日                   | ▲82%  | DB比較・証跡生成を自動化                     |
| **■ 合計（初回）**          | **165 人日**            | **55 人日**              | **▲110 人日（▲67%）** | 大半を自動化で削減                           |
| **■ 合計（2回目以降）**     | **160 人日**            | **47 人日**              | **▲113 人日（▲71%）** | 環境構築が不要になり削減幅がさらに拡大        |

■ ヘッドメッセージ（説明冒頭用）
「試験業務を構成する 5 工数を分解し、AI と自動化によって “どこが、どれだけ” 効くかを1枚の表にしました。初回は約 67%・2回目以降は約 71% の工数削減が期待でき、特に試験実施と検証が圧倒的に効率化されます。」

あなた:
良くなってきた。自動化をここ見ても無くならない人手作業は何か、逆に新たに必要な事で大きなものはコメントに追加

ChatGPT:
了解です。
「自動化しても残る“人手作業”」「逆に自動化で新たに必要になる“大きな作業”」をコメント欄に明記した決定版の表を作り直します。余計な情報を削ぎ落としつつ、説明力を最大化した 1 枚だけの勝負テーブルです。

**■ 試験業務の工数内訳と AI 導入効果（1,000 項目規模）
― 残る人手作業と新規作業を明確にした決定版表 ―**

| 工数区分                     | 人手（現状）    | AI・自動化後   | 削減率 | コメント（残る人手作業 / 新たに必要な作業）                                 |
|-----------------------------|------------------|-----------------|--------|--------------------------------------------------------------------------------|
| ① 試験項目作成              | 50 人日          | 25 人日         | ▲50%  | 【残】曖昧な仕様の判断・観点補完 / 【新】AIの生成結果のレビュー             |
| ② シナリオ化（順序/データ） | 10 人日          | 4 人日          | ▲60%  | 【残】例外系や業務特有ケースの判断 / 【新】AI生成シナリオの整合性チェック |
| ③ 自動化環境構築（初回）    | 5 人日           | 8 人日          | +3 人日| 【残】環境の許可・接続設定確認 / 【新】自動テスト基盤の初期セットアップ     |
| ④ 試験実施（画面操作）      | 67 人日          | 12 人日         | ▲82%  | 【残】探索的テスト・UIの“違和感”確認 / 【新】ケースの自動化対象判定        |
| ⑤ 検証（画面/DB/証跡）      | 33 人日          | 6 人日          | ▲82%  | 【残】業務観点の最終判定 / 【新】AI出力のエラー原因の切り分け・再実行       |
| **■ 合計（初回）**          | **165 人日**     | **55 人日**     | **▲67%** |                                                                            |
| **■ 合計（2回目以降）**     | **160 人日**     | **47 人日**     | **▲71%** | 初回構築後はメンテ中心だが、人手判断の領域は残る                             |

■ 補足：この表から“ひとことで”伝えられる要点
「自動化しても“仕様の解釈・探索・最終判断”は人手で残る。一方で“環境構築・自動化対象判定・AI出力の確認”という新しい工程が必要になる。しかし全体としては 67〜71% の削減余地がある。」

あなた:
もう一つだけ示したい。もしSI活用が失敗した場合、工数はどうなるか？

ChatGPT:
了解しました。
最後に 「SI（自動化）導入が失敗した場合の工数インパクト」 を、同じフォーマットの 1 枚表で示します。成功版と並べて説明することで、リスク・期待値のバランスを伝えられます。

**■ SI 活用が失敗した場合の工数（1,000 項目規模）
― “自動化がうまくいかないとこうなる” と明確に示す表 ―**

| 工数区分                     | 現状             | SI失敗時           | 差分（悪化） | コメント（何が起きるか）                                           |
|-----------------------------|------------------|----------------------|--------------|--------------------------------------------------------------------|
| ① 試験項目作成              | 50 人日          | 55 人日              | +5 人日      | 自動生成を当てにして準備した分、逆に見直し・やり直しが発生        |
| ② シナリオ化（順序/データ） | 10 人日          | 14 人日              | +4 人日      | 自動化前提の構造化が無駄になり、項目を手戻りで再整理              |
| ③ 自動化環境構築            | 5 人日           | 10 人日              | +5 人日      | 着手した自動化基盤が使えず“捨てコスト”になる                       |
| ④ 試験実施（画面操作）      | 67 人日          | 67 人日              | ±0           | 結局すべて手動に戻る                                               |
| ⑤ 検証（画面/DB/証跡）      | 33 人日          | 33 人日              | ±0           | 自動比較が使えないため現状のまま                                   |
| **■ 合計（初回）**          | **165 人日**     | **179 人日**         | **+14 人日** | 部分的に着手した分、むしろ工数が増える                             |
| **■ 合計（2回目以降）**     | **160 人日**     | **160 人日**         | **±0**       | 手戻りは一巡で収束、ただし投資分は回収できない                     |

■ ひとことで説明するなら
「SI自動化が失敗した場合は、初回のみ 1〜2 人月の余分な工数が出る。ただし業務の本流（試験実施・検証）は現状維持で、恒常的な悪化はしない。」

つまり、
“初回の投資リスク ≒ +14 人日（＋約 0.7 人月）” が最大損失。その代わり成功時は ▲110人日（▲67%）削減。

必要なら 成功・失敗を左右する判断ポイント や「絶対に失敗させないための最小スコープ」も 1 枚にまとめられます。
